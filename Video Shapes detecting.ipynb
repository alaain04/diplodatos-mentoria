{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(title, img):\n",
    "    cv2.imshow(title,img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton(src):\n",
    "    skel = np.zeros(src.shape,np.uint8)\n",
    "    size = np.size(src)\n",
    "    ret,img2 = cv2.threshold(src,127,255,0)\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))\n",
    "    done = False\n",
    "    \n",
    "    while( not done):\n",
    "        eroded = cv2.erode(img2,element)\n",
    "        temp = cv2.dilate(eroded,element)\n",
    "        temp = cv2.subtract(img2,temp)\n",
    "        skel = cv2.bitwise_or(skel,temp)\n",
    "        img2 = eroded.copy()\n",
    "    \n",
    "        zeros = size - cv2.countNonZero(img2)\n",
    "        if zeros==size:\n",
    "            done = True\n",
    "\n",
    "    return skel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./CLEANED_VIDEOS/' + 'adas.mp4')\n",
    "if (cap.isOpened()== False): \n",
    "      print(\"Error opening video stream or file\")\n",
    "i = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        kernel = np.ones((1,1),np.uint8)\n",
    "        #opening = cv2.morphologyEx(frame, cv2.MORPH_CLOSE, kernel)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "        \n",
    "        #EQUALIZED GRAYSCALE\n",
    "        clahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(5,5))\n",
    "        cl1 = clahe.apply(gray)\n",
    "                \n",
    "        #blur = cv2.blur(c, (5,5)) # blur the image\n",
    "        thresh = cv2.adaptiveThreshold(cl1, 255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,15,2)\n",
    "        \n",
    "        #canny_output = cv2.Canny(thresh, 50, 150,apertureSize=3)\n",
    "        skel = skeleton(thresh)\n",
    "        kernel = np.ones((3,1),np.uint8)\n",
    "        skel1 = cv2.dilate(skel, kernel, iterations=2)\n",
    "\n",
    "        kernel = np.ones((1,3),np.uint8)\n",
    "        skel2 = cv2.dilate(skel, kernel, iterations=2)\n",
    "\n",
    "        skel3 = cv2.bitwise_or(skel1, skel2)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, hierarchy = cv2.findContours(skel, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        \n",
    "        for cnt in contours:\n",
    "            approx = []\n",
    "            epsilon = 0.1*cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            #showImage('img', img)\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if (area > 1000 and area < 500000 and  len(approx) == 4):\n",
    "                i += 1\n",
    "                cv2.drawContours(frame, [approx], -1, (15, 254, 251), 2)\n",
    "                #cv2.imwrite('./CLEANED_VIDEOS/'  +'images/result_{}.png'.format(i),frame)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else: \n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-84c4bbba706f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mopening\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphologyEx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMORPH_CLOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshowImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENING'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "kernel = np.ones((1,1),np.uint8)\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "showImage('OPENING', opening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(opening, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "showImage('GRAY', gray)\n",
    "blur = cv2.blur(gray, (5,5)) # blur the image\n",
    "showImage('IBLUR', blur)\n",
    "thresh = cv2.adaptiveThreshold(blur, 255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "showImage('THRESH', thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAHE (Contrast Limited Adaptive Histogram Equalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(3,3))\n",
    "cl1 = clahe.apply(thresh)\n",
    "showImage('CLASHE',cl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and find contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#src_gray = cv2.blur(src_gray, (3,3))\n",
    "#showImage(src_gray)\n",
    "\n",
    "# Detect edges using Canny\n",
    "canny_output = cv2.Canny(cl1, 50, 150,apertureSize=3)\n",
    "showImage('CANNY', canny_output)\n",
    "\n",
    "# Find contours\n",
    "contours, hierarchy = cv2.findContours(canny_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing = np.zeros((canny_output.shape[0], canny_output.shape[1], 3), dtype=np.uint8)\n",
    "for cnt in contours:\n",
    "    approx = []\n",
    "    epsilon = 0.1*cv2.arcLength(cnt, True)\n",
    "    approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "    #showImage('img', img)\n",
    "    area = cv2.contourArea(cnt)\n",
    "\n",
    "    if (area > 10000 and len(approx) == 4):\n",
    "        print(area)\n",
    "        print('cont', len(approx))\n",
    "        cv2.drawContours(img, [approx], -1, (15, 254, 251), 2)\n",
    "        showImage('CONTOURS', img)\n",
    "        #break\n",
    "    ##    \n",
    "    #    x = approx.ravel()[0]\n",
    "    #    y = approx.ravel()[1]\n",
    "    #    cv2.putText(img, str(area), (x, y), cv2.FONT_HERSHEY_SIMPLEX , 1, (35))\n",
    "    #    #cv2.imwrite(clean_videos_path  +'images/screenshot_res.png',img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalization image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.0) /io/opencv/modules/imgproc/src/histogram.cpp:3334: error: (-215:Assertion failed) _src.type() == CV_8UC1 in function 'equalizeHist'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-f3a2c65a59d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mequ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalizeHist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshowImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INITIAL IMAGE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshowImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EQUALIZATION'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mequ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.0.0) /io/opencv/modules/imgproc/src/histogram.cpp:3334: error: (-215:Assertion failed) _src.type() == CV_8UC1 in function 'equalizeHist'\n"
     ]
    }
   ],
   "source": [
    "equ = cv2.equalizeHist(img)\n",
    "showImage('INITIAL IMAGE', img)\n",
    "showImage('EQUALIZATION',equ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.GaussianBlur(img, (11, 11), 0)\n",
    "sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0)\n",
    "sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1)\n",
    "laplacian = cv2.Laplacian(img, cv2.CV_64F, ksize=5)\n",
    "canny = cv2.Canny(img, 100, 150)\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imshow(\"Sobelx\", sobelx)\n",
    "cv2.imshow(\"Sobely\", sobely)\n",
    "cv2.imshow(\"Laplacian\", laplacian)\n",
    "cv2.imshow(\"Canny\", canny)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in files[:1]:\n",
    "file = 'canal12__03-05-2019__09-30.avi'\n",
    "cap = cv2.VideoCapture(clean_videos_path + file)\n",
    "i = 0\n",
    "while(cap.isOpened()):\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        # Convert image to gray and blur it\n",
    "        src_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        src_gray = cv2.blur(src_gray, (3,3))\n",
    "        \n",
    "        # Detect edges using Canny\n",
    "        canny_output = cv2.Canny(src_gray, 100, 100 * 2)\n",
    "        # Find contours\n",
    "        contours, hierarchy = cv2.findContours(canny_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw contours\n",
    "        #drawing = np.zeros((canny_output.shape[0], canny_output.shape[1], 3), dtype=np.uint8)\n",
    "        for cnt in contours:\n",
    "            approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt, True), True)\n",
    "            \n",
    "            area = cv2.contourArea(cnt)\n",
    "            if len(approx) == 4 and area > 20:\n",
    "                cv2.drawContours(frame, [approx], 0, (0), 2)\n",
    "                x = approx.ravel()[0]\n",
    "                y = approx.ravel()[1]\n",
    "                cv2.putText(frame, str(area), (x, y), cv2.FONT_HERSHEY_SIMPLEX , 3, (2))\n",
    "                cv2.imwrite(clean_videos_path  +'images/result_{}.png'.format(i),frame)\n",
    "                cv2.imshow(\"shapes\", frame)\n",
    "            #cv2.imshow(\"Threshold\", frame)\n",
    "        # Show in a window\n",
    "        #cv2.imshow('Contours', drawing)\n",
    "        #Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    # Break the loop\n",
    "    else: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
